# -*- coding: utf-8 -*-
"""Salman Alsehli ML_Task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qNXz9kK27jNJJUhejWCU3LfUqMs-7TD9
"""

##Importing libraries
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn import preprocessing
from lightgbm import LGBMClassifier

from google.colab import drive
drive.mount('/content/drive')

#Loading the data
client = pd.read_csv("/content/drive/MyDrive/client_train.csv")
invoice = pd.read_csv("/content/drive/MyDrive/invoice_train.csv", low_memory=False)
Testclient = pd.read_csv("/content/drive/MyDrive/client_test.csv")
Testinvoice =  pd.read_csv("/content/drive/MyDrive/invoice_test.csv")
SampleSubmission =  pd.read_csv("/content/drive/MyDrive/SampleSubmission.csv")

"""<h1 style="color: blue;">1. Data understanding</h1>"""

client.info()

invoice.info()

client.describe()

invoice.describe()

client.head()

invoice.head()

"""## what is found?

* Both invoice and client have a column in common(client_id) .

> which could be merged into one dataFrame
<hr>
<h1 style="color: blue;">2. Data cleaning </h1>
"""

#Merge the data in one dataframe on client_id column!
train = pd.merge(client,invoice , on="client_id")
test = pd.merge(Testclient,Testinvoice , on="client_id")

#cheacking for null values

train.isnull().sum()

#train = train.drop(columns=['counter_statue'], axis=1)

# Initialize unique_category variable before the loop
unique_category = 0

# Decide which categorical variables you want to use in the model
for col_name in client.columns:
    if client[col_name].dtypes == 'object':
        unique_category = len(client[col_name].unique())
    print("The Features of '{col_name}' has a {unique_category} unique categories".format(col_name=col_name, unique_category=unique_category))

# Initialize unique_category variable before the loop
unique_category = 0

# Decide which categorical variables you want to use in the model
for col_name in invoice.columns:
    if invoice[col_name].dtypes == 'object':
        unique_category = len(invoice[col_name].unique())
    print("The Features of '{col_name}' has a {unique_category} unique categories".format(col_name=col_name, unique_category=unique_category))

for col in ['target','disrict','region','client_catg']:
    viz = client.groupby([col])['client_id'].count()
    plt.bar(x=viz.index, height=viz.values)
    plt.title(col+' distribution')
    plt.show()

cm = plt.figure(figsize=(19, 15))
plt.matshow(train.corr(), fignum=cm.number)
plt.xticks(range(train.select_dtypes(['number']).shape[1]), train.select_dtypes(['number']).columns, fontsize=12, rotation=45)
plt.yticks(range(train.select_dtypes(['number']).shape[1]), train.select_dtypes(['number']).columns, fontsize=12)
cb = plt.colorbar()
cb.ax.tick_params(labelsize=20)
plt.title('Correlation Matrix', fontsize=16);

#prepare the dtype.

#client_id
from sklearn.preprocessing import OrdinalEncoder
OE = OrdinalEncoder()
train["client_id"] = OE.fit_transform(train[["client_id"]])
test["client_id"] = OE.fit_transform(test[["client_id"]])

#counter_type
omet={'ELEC':0,"GAZ":1}
train['counter_type']=train["counter_type"].replace(omet)
test['counter_type']=test["counter_type"].replace(omet)
pd.to_numeric(train['counter_type'])
pd.to_numeric(test['counter_type'])

print(train['counter_type'].value_counts())

#counter_statue
omes={'A':6,'0':0,'1':1,'5':5,'4':4}
train['counter_statue']=train["counter_statue"].replace(omes)
test['counter_statue']=test["counter_statue"].replace(omes)

print(train['counter_statue'].value_counts())

#converting invoice_date and creation_date datatype to datetime
#Extract Year, Month, and Day from both


train['invoice_date'] = pd.to_datetime(train['invoice_date'])
test['invoice_date'] = pd.to_datetime(test['invoice_date'])
train['creation_date'] = pd.to_datetime(train['creation_date'])
test['creation_date'] = pd.to_datetime(test['creation_date'])

#invoice_date

train['invoice_date']=pd.to_datetime(train['invoice_date'])
train['day'] = train['invoice_date'].dt.day
train['month'] = train['invoice_date'].dt.month
train['year'] = train['invoice_date'].dt.year
train['invoice_date']=train['day']+train['month']*100+train['year']*10000
train=train.drop(['day','month','year'],axis=1)

test['invoice_date']=pd.to_datetime(test['invoice_date'])
test['day'] = test['invoice_date'].dt.day
test['month'] = test['invoice_date'].dt.month
test['year'] = test['invoice_date'].dt.year
test['invoice_date']=test['day']+test['month']*100+test['year']*10000
test=test.drop(['day','month','year'],axis=1)

#creation_date
train['creation_date']=pd.to_datetime(train['creation_date'])
train['day'] = train['creation_date'].dt.day
train['month'] = train['creation_date'].dt.month
train['year'] = train['creation_date'].dt.year
train['creation_date']=train['day']+train['month']*100+train['year']*10000
train=train.drop(['day','month','year'],axis=1)


test['creation_date']=pd.to_datetime(test['creation_date'])
test['day'] = test['creation_date'].dt.day
test['month'] = test['creation_date'].dt.month
test['year'] = test['creation_date'].dt.year
test['creation_date']=test['day']+test['month']*100+test['year']*10000
test=test.drop(['day','month','year'],axis=1)

train['invoice_date']

"""<h1 style="color: blue;">3. Data modeling</h1>"""

# Commented out IPython magic to ensure Python compatibility.
lgbm = LGBMClassifier(boosting_type='gbdt',num_iterations=500, silent=True)

x_train = train.drop(columns=['target'], axis=1)
y_train = train['target']

# Convert 'counter_statue' column to a numeric data type (category or numeric)
x_train['counter_statue'] = pd.to_numeric(x_train['counter_statue'], errors='coerce')  # Coerce non-numeric values to NaN

# You can use one-hot encoding if the 'counter_statue' column contains non-numeric categorical values
# x_train = pd.get_dummies(x_train, columns=['counter_statue'])

# Create and fit the LGBMClassifier
lgbm = LGBMClassifier(boosting_type='gbdt', num_iterations=500, silent=True)
# %time lgbm.fit(x_train, y_train)

# Now the model is fitted and you can make predictions
# Assuming you have loaded your test dataset into the 'test' DataFrame
x_test = test

predict = lgbm.predict_proba(x_test)

predict = pd.DataFrame(predict)

predict.head()

submission = pd.DataFrame({
        "client_id": SampleSubmission["client_id"],
        "target": predict[0]
    })
submission.to_csv('sam_sub_0.csv', index=False)

submission.head()

submission = pd.DataFrame({
        "client_id": SampleSubmission["client_id"],
        "target": predict[1]
    })
submission.to_csv('sam_sub_1.csv', index=False)

submission.head()